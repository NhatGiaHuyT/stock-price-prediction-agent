# -*- coding: utf-8 -*-
"""text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JqavtIx21G5UPJnhyQPbafd7xQEShiWZ
"""

!pip install nltk

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

df1=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/nasdaq technocolabs/Untitled Folder/Predicting-Stock-Price-Changes-Using-Past-Prices-and-News-Articles-ML1-main/Datasets/Market Headlines/News_articles_dataset.csv",parse_dates=['Date'])

df1.head()

df1.isnull().sum()

df1.sort_index(inplace=True)

df1.head(10)

df=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/nasdaq technocolabs/Untitled Folder/Predicting-Stock-Price-Changes-Using-Past-Prices-and-News-Articles-ML1-main/Datasets/Nasdaq Stocks/HistoricalData_APPLE.csv",parse_dates=['Date'])

df.head(10)

df.sort_values('Date',inplace=True)

df.reset_index()

df.columns

a=pd.merge(df1,df,how='inner',on=['Date'])

a.head()

a.tail()

a["Close/Last"]=a["Close/Last"].str.replace('$', '').astype(float)
a["Open"]=a["Open"].str.replace('$', '').astype(float)
a["High"]=a["High"].str.replace('$', '').astype(float)
a["Low"]=a["Low"].str.replace('$', '').astype(float)

a.head(10)

a.index=a['Date']

a.head()

a.drop('Date',axis=1,inplace=True)

a.head()

a.sort_index(ascending=True,inplace=True)

a.head(10)

import os

a.to_csv('News and Historical data append.csv')

os.getcwd()

a['compound'] = ''
a['negative'] = ''
a['neutral'] = ''
a['positive'] = ''
a.head()

import nltk
nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer
import unicodedata

# instantiating the Sentiment Analyzer
sid = SentimentIntensityAnalyzer()

# calculating sentiment scores
a['compound'] = a['Headlines'].apply(lambda x: sid.polarity_scores(x)['compound'])
a['negative'] = a['Headlines'].apply(lambda x: sid.polarity_scores(x)['neg'])
a['neutral'] = a['Headlines'].apply(lambda x: sid.polarity_scores(x)['neu'])
a['positive'] = a['Headlines'].apply(lambda x: sid.polarity_scores(x)['pos'])

# displaying the stock data
a.head()

a.columns

#a.drop('Headlines', inplace=True, axis=1)

# rearranging the columns of the whole stock_data
a = a[['Close/Last', 'compound', 'negative', 'neutral', 'positive', 'Open', 'High', 'Low', 'Volume']]

# displaying the final stock_data
a.head()

a.to_csv('stock_data.csv')

stock_data = pd.read_csv('/content/stock_data.csv', index_col = False)

# renaming the column
stock_data.rename(columns={'Unnamed: 0':'Date'}, inplace = True)

# setting the column 'Date' as the index column
stock_data.set_index('Date', inplace=True)

# displaying the stock_data
stock_data.head()

stock_data.shape

stock_data.isna().any()

stock_data.describe(include='all')

stock_data.info()

# calculating data_to_use
percentage_of_data = 1.0
data_to_use = int(percentage_of_data*(len(stock_data)-1))

# using 80% of data for training
train_end = int(data_to_use*0.8)
total_data = len(stock_data)
start = total_data - data_to_use

# printing number of records in the training and test datasets
print("Number of records in Training Data:", train_end)
print("Number of records in Test Data:", total_data - train_end)

# predicting one step ahead
steps_to_predict = 1

# capturing data to be used for each column
close_price = stock_data.iloc[start:total_data,0] #close
compound = stock_data.iloc[start:total_data,1] #compound
negative = stock_data.iloc[start:total_data,2] #neg
neutral = stock_data.iloc[start:total_data,3] #neu
positive = stock_data.iloc[start:total_data,4] #pos
open_price = stock_data.iloc[start:total_data,5] #open
high = stock_data.iloc[start:total_data,6] #high
low = stock_data.iloc[start:total_data,7] #low
volume = stock_data.iloc[start:total_data,8] #volume

# printing close price
print("Close Price:")
close_price

# shifting next day close
close_price_shifted = close_price.shift(-1) 

# shifting next day compound
compound_shifted = compound.shift(-1) 

# concatenating the captured training data into a dataframe
data = pd.concat([close_price, close_price_shifted, compound, compound_shifted, volume, open_price, high, low], axis=1)

# setting column names of the revised stock data
data.columns = ['close_price', 'close_price_shifted', 'compound', 'compound_shifted','volume', 'open_price', 'high', 'low']

# dropping nulls
data = data.dropna()    
data.head(10)

y = data['close_price_shifted']
y

# setting the features dataset for prediction  
cols = ['close_price', 'compound', 'compound_shifted', 'volume', 'open_price', 'high', 'low']
x = data[cols]
x

from sklearn import preprocessing, metrics
from sklearn.preprocessing import MinMaxScaler

# scaling the feature dataset
scaler_x = preprocessing.MinMaxScaler (feature_range=(-1, 1))
x = np.array(x).reshape((len(x) ,len(cols)))
x = scaler_x.fit_transform(x)

# scaling the target variable
scaler_y = preprocessing.MinMaxScaler (feature_range=(-1, 1))
y = np.array (y).reshape ((len( y), 1))
y = scaler_y.fit_transform (y)

# displaying the scaled feature dataset and the target variable
x, y

# preparing training and test dataset
X_train = x[0 : train_end,]
X_test = x[train_end+1 : len(x),]    
y_train = y[0 : train_end] 
y_test = y[train_end+1 : len(y)]  

# printing the shape of the training and the test datasets
print('Number of rows and columns in the Training set X:', X_train.shape, 'and y:', y_train.shape)
print('Number of rows and columns in the Test set X:', X_test.shape, 'and y:', y_test.shape)

# reshaping the feature dataset for feeding into the model
X_train = X_train.reshape (X_train.shape + (1,)) 
X_test = X_test.reshape(X_test.shape + (1,))

# printing the re-shaped feature dataset
print('Shape of Training set X:', X_train.shape)
print('Shape of Test set X:', X_test.shape)

from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Dense, Activation

# setting the seed to achieve consistent and less random predictions at each execution
np.random.seed(500)

# setting the model architecture
model=Sequential()
model.add(LSTM(200,return_sequences=True,activation='tanh',input_shape=(len(cols),1)))
model.add(Dropout(0.4))
model.add(LSTM(250,activation='tanh'))
model.add(Dropout(0.4))
model.add(Dense(1))

# printing the model summary
model.summary()

# compiling the model
model.compile(loss='mse' , optimizer='adam')

# fitting the model using the training dataset
model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=16, verbose=1)

# saving the model as a json file
model_json = model.to_json()
with open('model.json', 'w') as json_file:
    json_file.write(model_json)
    
# serialize weights to HDF5
model.save_weights('model.h5')
print('Model is saved to the disk')

# performing predictions
predictions = model.predict(X_test) 

# unscaling the predictions
predictions = scaler_y.inverse_transform(np.array(predictions).reshape((len(predictions), 1)))

# printing the predictions
print('Predictions:')
predictions[0:5]

# calculating the training mean-squared-error
train_loss = model.evaluate(X_train, y_train, batch_size = 1)

# calculating the test mean-squared-error
test_loss = model.evaluate(X_test, y_test, batch_size = 1)

# printing the training and the test mean-squared-errors
print('Train Loss =', round(train_loss,4))
print('Test Loss =', round(test_loss,4))

# calculating root mean squared error
root_mean_square_error = np.sqrt(np.mean(np.power((y_test - predictions),2)))
print('Root Mean Square Error =', round(root_mean_square_error,4))

# calculating root mean squared error using sklearn.metrics package
rmse = metrics.mean_squared_error(y_test, predictions)
print('Root Mean Square Error (sklearn.metrics) =', round(np.sqrt(rmse),4))

# unscaling the test feature dataset, x_test
X_test = scaler_x.inverse_transform(np.array(X_test).reshape((len(X_test), len(cols))))

# unscaling the test y dataset, y_test
y_train = scaler_y.inverse_transform(np.array(y_train).reshape((len(y_train), 1)))
y_test = scaler_y.inverse_transform(np.array(y_test).reshape((len(y_test), 1)))

# plotting
plt.figure(figsize=(16,10))

# plt.plot([row[0] for row in y_train], label="Training Close Price")
plt.plot(predictions, label="Predicted Close Price")
plt.plot([row[0] for row in y_test], label="Testing Close Price")
plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2)
plt.show()

